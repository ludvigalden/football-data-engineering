{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Football Match Outcome Prediction Model\n",
        "\n",
        "This notebook demonstrates training a simple prediction model for football match outcomes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"src\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting Spark log level to \"ERROR\".\n"
          ]
        }
      ],
      "source": [
        "from spark_session import create_spark_session\n",
        "\n",
        "spark = create_spark_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time for load_parquet_data: 0.14 seconds\n"
          ]
        }
      ],
      "source": [
        "from data_loader import get_data_path, load_parquet_data\n",
        "\n",
        "matches_transformed_df, load_time = load_parquet_data(spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----------------+----------+-------+-------+-------+---------+---------+--------+\n",
            "|      Date|        HomeTeam|  AwayTeam|HomeElo|AwayElo|EloDiff|Form3Home|Form3Away|FTResult|\n",
            "+----------+----------------+----------+-------+-------+-------+---------+---------+--------+\n",
            "|2023-01-21|        Coventry|   Norwich|1514.06|1557.43| -43.37|      2.0|      4.0|       A|\n",
            "|2023-02-04|         Norwich|   Burnley|1568.72|1718.49|-149.77|      6.0|      9.0|       A|\n",
            "|2023-02-11|         Burnley|   Preston|1718.49|1503.04| 215.45|      9.0|      3.0|       H|\n",
            "|2023-02-21|         Norwich|Birmingham|1556.65|1446.99| 109.66|      4.0|      3.0|       H|\n",
            "|2023-03-11|Sheffield United|     Luton| 1624.2|1560.28|  63.92|      6.0|      7.0|       A|\n",
            "+----------+----------------+----------+-------+-------+-------+---------+---------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "matches_transformed_df.select(\n",
        "    \"Date\", \"HomeTeam\", \"AwayTeam\", \"HomeElo\", \"AwayElo\", \"EloDiff\", \"Form3Home\", \"Form3Away\", \"FTResult\"\n",
        ").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time for prepare_features: 0.41 seconds\n"
          ]
        }
      ],
      "source": [
        "from ml import prepare_features\n",
        "\n",
        "(matches_features_df, prep_time) = prepare_features(matches_transformed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------+-----+--------+\n",
            "|Features                         |Label|FTResult|\n",
            "+---------------------------------+-----+--------+\n",
            "|[1514.06,1557.43,-43.37,2.0,4.0] |1.0  |A       |\n",
            "|[1568.72,1718.49,-149.77,6.0,9.0]|1.0  |A       |\n",
            "|[1718.49,1503.04,215.45,9.0,3.0] |0.0  |H       |\n",
            "|[1556.65,1446.99,109.66,4.0,3.0] |0.0  |H       |\n",
            "|[1624.2,1560.28,63.92,6.0,7.0]   |1.0  |A       |\n",
            "+---------------------------------+-----+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "matches_features_df.select(\"Features\", \"Label\", \"FTResult\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time for prepare_features: 0.31 seconds\n",
            "Execution time for train_and_evaluate: 2.32 seconds\n",
            "Test size: 0.1, Accuracy: 0.4886, Training time: 2.32s\n",
            "Execution time for prepare_features: 0.22 seconds\n",
            "Execution time for train_and_evaluate: 1.60 seconds\n",
            "Test size: 0.2, Accuracy: 0.4977, Training time: 1.60s\n",
            "Execution time for prepare_features: 0.15 seconds\n",
            "Execution time for train_and_evaluate: 1.53 seconds\n",
            "Test size: 0.3, Accuracy: 0.4967, Training time: 1.53s\n",
            "Execution time for prepare_features: 0.19 seconds\n",
            "Execution time for train_and_evaluate: 1.68 seconds\n",
            "Test size: 0.4, Accuracy: 0.4971, Training time: 1.68s\n",
            "Execution time for prepare_features: 0.40 seconds\n",
            "Execution time for train_and_evaluate: 1.99 seconds\n",
            "Test size: 0.5, Accuracy: 0.4959, Training time: 1.99s\n"
          ]
        }
      ],
      "source": [
        "from ml import train_and_evaluate\n",
        "\n",
        "# Train model with different test sizes to evaluate stability\n",
        "test_sizes = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "results = []\n",
        "\n",
        "\n",
        "for test_size in test_sizes:\n",
        "\n",
        "    (model, predictions, accuracy), train_time = train_and_evaluate(matches_transformed_df, test_size)\n",
        "\n",
        "    results.append({\"TestSize\": test_size, \"Accuracy\": accuracy, \"TrainingTime\": train_time})\n",
        "\n",
        "    print(f\"Test size: {test_size}, Accuracy: {accuracy:.4f}, Training time: {train_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----------+-----+\n",
            "|FTResult|Prediction|count|\n",
            "+--------+----------+-----+\n",
            "|       A|       0.0|11373|\n",
            "|       A|       1.0| 7732|\n",
            "|       D|       0.0|13721|\n",
            "|       D|       1.0| 4390|\n",
            "|       H|       0.0|25870|\n",
            "|       H|       1.0| 4669|\n",
            "+--------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prediction_counts = predictions.groupBy(\"FTResult\", \"Prediction\").count().orderBy(\"FTResult\", \"Prediction\")\n",
        "prediction_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+-----+--------+\n",
            "|League|Correct|Total|Accuracy|\n",
            "+------+-------+-----+--------+\n",
            "|   SC2|     10|   14|    0.71|\n",
            "|    EC|     13|   20|    0.65|\n",
            "|    N1|   1853| 3369|    0.55|\n",
            "|    G1|   1002| 1836|    0.55|\n",
            "|    E0|   2458| 4594|    0.54|\n",
            "|    P1|   1704| 3184|    0.54|\n",
            "|   SP1|   2374| 4520|    0.53|\n",
            "|    I1|   2370| 4459|    0.53|\n",
            "|    T1|   1690| 3208|    0.53|\n",
            "|   SC0|   1219| 2305|    0.53|\n",
            "|   AUT|    177|  331|    0.53|\n",
            "|    B1|   1492| 2880|    0.52|\n",
            "|   NOR|    563| 1074|    0.52|\n",
            "|    D1|   1860| 3667|    0.51|\n",
            "|   SWE|    221|  430|    0.51|\n",
            "|   RUS|     53|  104|    0.51|\n",
            "|   ROM|     40|   80|     0.5|\n",
            "|    F1|   2143| 4382|    0.49|\n",
            "|   FIN|     55|  115|    0.48|\n",
            "|    D2|   1615| 3469|    0.47|\n",
            "+------+-------+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "league_accuracy = (\n",
        "    predictions.groupBy(\"League\")\n",
        "    .agg(F.count(F.when(F.col(\"Prediction\") == F.col(\"Label\"), 1)).alias(\"Correct\"), F.count(\"*\").alias(\"Total\"))\n",
        "    .withColumn(\"Accuracy\", F.round(F.col(\"Correct\") / F.col(\"Total\"), 2))\n",
        "    .orderBy(F.desc(\"Accuracy\"))\n",
        ")\n",
        "\n",
        "league_accuracy.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+-------------------+\n",
            "|HomeWins| Total|   BaselineAccuracy|\n",
            "+--------+------+-------------------+\n",
            "|   61265|135836|0.45102182043051914|\n",
            "+--------+------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare with baseline (always predict home win)\n",
        "baseline = matches_transformed_df.agg(\n",
        "    F.count(F.when(F.col(\"FTResult\") == \"H\", 1)).alias(\"HomeWins\"), F.count(\"*\").alias(\"Total\")\n",
        ").withColumn(\"BaselineAccuracy\", F.col(\"HomeWins\") / F.col(\"Total\"))\n",
        "\n",
        "baseline.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature importance (coefficients):\n",
            "[[ 1.00557815e-03 -8.02856329e-04  2.28791562e-03  9.07090235e-03\n",
            "  -1.16881855e-02]\n",
            " [-9.06419416e-04  8.54309802e-04 -2.22745243e-03 -1.16825840e-02\n",
            "   3.95321738e-03]\n",
            " [-9.91587324e-05 -5.14534722e-05 -6.04631912e-05  2.61168162e-03\n",
            "   7.73496817e-03]]\n"
          ]
        }
      ],
      "source": [
        "# Feature importance (coefficients from logistic regression)\n",
        "feature_importance = model.coefficientMatrix.toArray()\n",
        "print(\"Feature importance (coefficients):\")\n",
        "print(feature_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to data/models/outcome_lr\n"
          ]
        }
      ],
      "source": [
        "model_path = get_data_path(\"data/models/outcome_lr\")\n",
        "model.write().overwrite().save(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
